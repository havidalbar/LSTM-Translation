{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM Translation Spain to Eng",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU6tnqrN52z9",
        "colab_type": "code",
        "outputId": "80fa9e88-e485-4cb1-972a-d46b05bf0315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import string\n",
        "import math\n",
        "import re\n",
        "from pickle import dump,load\n",
        "from unicodedata import normalize\n",
        "from numpy import array,argmax\n",
        "from numpy.random import rand,shuffle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import LSTM,Dense,Embedding,RepeatVector,TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "from nltk.translate.bleu_score import corpus_bleu,SmoothingFunction\n",
        "from keras import backend as K\n",
        "K.tensorflow_backend._get_available_gpus()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtCplPQ-8iVT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b971ca25-9966-423c-b50f-fa28645e1ea4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVjgY8d856M8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Preprocessing:\n",
        "    def __init__(self, filename='translation_english.txt'):\n",
        "        self.document = self.load_doc(filename)\n",
        "        self.clean_document = ''\n",
        "\n",
        "    def load_doc(self, filename):\n",
        "        file = open(filename, mode='rt', encoding='utf-8')\n",
        "        text = file.read()\n",
        "        file.close()\n",
        "        return text\n",
        "\n",
        "    def clean_pairs(self, lines):\n",
        "        cleaned = list()\n",
        "        re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "        table = str.maketrans('', '', string.punctuation)\n",
        "        for pair in lines:\n",
        "            clean_pair = list()\n",
        "            for line in pair:\n",
        "                line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "                line = line.decode('UTF-8')\n",
        "                line = line.lower().split()\n",
        "                line = [word.translate(table) for word in line]\n",
        "                line = [re_print.sub('', word) for word in line]\n",
        "                line = [word for word in line if word.isalpha()]\n",
        "                clean_pair.append(' '.join(line))\n",
        "            cleaned.append(clean_pair)\n",
        "        self.clean_document = array(cleaned)\n",
        "        return array(cleaned)\n",
        "\n",
        "    def get_clean_pairs(self):\n",
        "        return self.clean_document\n",
        "\n",
        "    def to_pairs(self):\n",
        "        lines = self.document.strip().split('\\n')\n",
        "        pairs = [line.split('\\t') for line in  lines]\n",
        "        return pairs\n",
        "\n",
        "    def save_clean_data(self, document, filename='english-spanish.pkl'):\n",
        "        dump(document, open(filename, 'wb'))\n",
        "        print('Saved: %s' % filename)\n",
        "\n",
        "    def preprocess(self):\n",
        "        self.save_clean_data(\n",
        "            self.clean_pairs(\n",
        "                self.to_pairs()\n",
        "            )\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-9WxEPg5_mA",
        "colab_type": "code",
        "outputId": "0df91575-6cae-4133-f295-406eb0a45a51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "preprocess = Preprocessing()\n",
        "preprocess.preprocess()\n",
        "for i in range(900,1000):\n",
        "\tprint('[%s] => [%s]' % (preprocess.get_clean_pairs()[i,0], preprocess.get_clean_pairs()[i,1]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-spanish.pkl\n",
            "[be careful] => [ten cuidado]\n",
            "[be careful] => [se cuidadoso]\n",
            "[be content] => [estate contento]\n",
            "[be on time] => [llega a tiempo]\n",
            "[be on time] => [llegue a tiempo]\n",
            "[be patient] => [sea paciente]\n",
            "[be serious] => [se serio]\n",
            "[birds sing] => [los pajaros cantan]\n",
            "[birds sing] => [los pajaros estan cantando]\n",
            "[bring food] => [traed comida]\n",
            "[bring help] => [traed ayuda]\n",
            "[bring wine] => [trae vino]\n",
            "[can i come] => [puedo ir]\n",
            "[can i come] => [puedo venir]\n",
            "[can i come] => [puedo acercarme]\n",
            "[can i help] => [puedo ayudar]\n",
            "[can i stay] => [me puedo quedar]\n",
            "[carry this] => [lleva esto]\n",
            "[check that] => [comprobad eso]\n",
            "[check this] => [comprueba esto]\n",
            "[choose one] => [escoge uno]\n",
            "[come again] => [vuelve otra vez]\n",
            "[come alone] => [ven solo]\n",
            "[come along] => [vente]\n",
            "[come along] => [venganse]\n",
            "[come early] => [veni temprano]\n",
            "[come early] => [ven temprano]\n",
            "[come early] => [vengan temprano]\n",
            "[come early] => [venga temprano]\n",
            "[come on in] => [pasale]\n",
            "[come on in] => [pasele]\n",
            "[come on in] => [pasenle]\n",
            "[come on in] => [entre]\n",
            "[come on in] => [pase]\n",
            "[come quick] => [ven rapido]\n",
            "[come quick] => [veni rapido]\n",
            "[come to me] => [ven a mi]\n",
            "[come to me] => [venid a mi]\n",
            "[come to us] => [ven a nosotros]\n",
            "[come to us] => [venid a nosotros]\n",
            "[cut it out] => [ya parale]\n",
            "[did tom go] => [fue tom]\n",
            "[do come in] => [pasale]\n",
            "[do come in] => [pasele]\n",
            "[do come in] => [pasenle]\n",
            "[do come in] => [pasa adentro]\n",
            "[do come in] => [entra de una vez]\n",
            "[do come in] => [entra ya]\n",
            "[do come in] => [metete dentro]\n",
            "[do men cry] => [los hombres lloran]\n",
            "[dont come] => [no vengas]\n",
            "[dont jump] => [no salteis]\n",
            "[dont look] => [no mireis]\n",
            "[dont move] => [no os movais]\n",
            "[dont move] => [no te muevas]\n",
            "[dont move] => [no se mueva]\n",
            "[dont move] => [no se muevan]\n",
            "[dont sing] => [no cantes]\n",
            "[dont sing] => [no canten]\n",
            "[dont stop] => [no pares]\n",
            "[dont talk] => [no hables]\n",
            "[dont talk] => [no hables]\n",
            "[dont wait] => [no esperes]\n",
            "[dont wait] => [no esperen]\n",
            "[dont wait] => [no esperes]\n",
            "[dont yell] => [no grites]\n",
            "[eat slowly] => [come despacio]\n",
            "[eat slowly] => [come despacio]\n",
            "[fire burns] => [el fuego quema]\n",
            "[follow tom] => [seguilo a tomas]\n",
            "[follow tom] => [siguelo a tomas]\n",
            "[follow tom] => [sigalo a tomas]\n",
            "[follow tom] => [siganlo a tomas]\n",
            "[follow him] => [siguele]\n",
            "[follow him] => [siguelo]\n",
            "[forget tom] => [olvidate de tomas]\n",
            "[forget tom] => [olvidate de tomas]\n",
            "[forget tom] => [olvidese de tomas]\n",
            "[forget him] => [olvidenlo]\n",
            "[forgive us] => [perdonanos]\n",
            "[forgive us] => [perdonenos]\n",
            "[get a life] => [consiguete una vida]\n",
            "[get inside] => [entra]\n",
            "[get to bed] => [vete a la cama]\n",
            "[give it up] => [dejalo]\n",
            "[go on home] => [vete a casa]\n",
            "[go on home] => [vayase a casa]\n",
            "[go see tom] => [ve a ver a tom]\n",
            "[go to work] => [anda a trabajar]\n",
            "[god exists] => [dios existe]\n",
            "[have faith] => [ten fe]\n",
            "[have faith] => [tened fe]\n",
            "[have faith] => [tengan fe]\n",
            "[have faith] => [confien]\n",
            "[he ate out] => [el salio a comer]\n",
            "[he coughed] => [tosio]\n",
            "[he gave in] => [el se rindio]\n",
            "[he gave up] => [se rindio]\n",
            "[he gave up] => [lo dejo]\n",
            "[he gave up] => [cedio]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkGW4Pd06MxR",
        "colab_type": "code",
        "outputId": "aeb58ab7-fd64-461a-bc24-ef74441e32b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "class TrainMachineTranslation:\n",
        "    def __init__(self, file_dataset = 'english-spanish.pkl'):\n",
        "        self.dataset = self.load_data(file_dataset)\n",
        "        self.eng_vocab_size = 0\n",
        "        self.eng_length = 0\n",
        "        self.spain_vocab_size = 0\n",
        "        self.spain_length = 0\n",
        "        self.train = ''\n",
        "        self.test = ''\n",
        "        self.trainX = ''\n",
        "        self.trainY = ''\n",
        "        self.testX = ''\n",
        "        self.testY = ''\n",
        "        self.main_train()\n",
        "\n",
        "    def split_dataset(self):\n",
        "        raw_dataset = self.dataset\n",
        "        shuffle(raw_dataset)\n",
        "        new_dataset = []\n",
        "        for i in raw_dataset:\n",
        "            if len(i[1].split(\" \")) <= 4 and len(i[0].split(\" \")) <= 4:\n",
        "                new_dataset.append([i[0],i[1]])\n",
        "        new_dataset = array(new_dataset)\n",
        "        n_sentences = len(new_dataset)\n",
        "        self.dataset = new_dataset[:n_sentences, :]\n",
        "        shuffle(self.dataset)\n",
        "        split = math.floor(len(self.dataset) - (len(self.dataset)*0.2))\n",
        "        self.train, self.test = self.dataset[:split], self.dataset[split:]\n",
        "\n",
        "    def load_data(self, filename):\n",
        "\t      return load(open(filename, 'rb'))\n",
        "\n",
        "    def save_clean_data(self, filename_dataset = 'english-spanish-both.pkl', filename_test = 'english-spanish-test.pkl', filename_train = 'english-spanish-train.pkl'):\n",
        "        dump(self.dataset, open(filename_dataset, 'wb'))\n",
        "        dump(self.train, open(filename_train, 'wb'))\n",
        "        dump(self.test, open(filename_test, 'wb'))\n",
        "        print('Saved: ', filename_dataset, filename_train, filename_test)\n",
        "\n",
        "    def create_tokenizer(self, lines):\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(lines)\n",
        "        return tokenizer\n",
        "\n",
        "    def max_length(self, lines):\n",
        "        return max(len(line.split()) for line in lines)\n",
        "\n",
        "    def encode_sequences(self, tokenizer, length, lines):\n",
        "        X = tokenizer.texts_to_sequences(lines)\n",
        "        X = pad_sequences(X, maxlen=length, padding='post')\n",
        "        return X\n",
        " \n",
        "    def encode_output(self, sequences, vocab_size):\n",
        "        ylist = [to_categorical(sequence, num_classes=vocab_size) for sequence in sequences]\n",
        "        y = array(ylist).reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "        return y\n",
        "\n",
        "    def define_model(self, src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "        model.add(LSTM(n_units))\n",
        "        model.add(RepeatVector(tar_timesteps))\n",
        "        model.add(LSTM(n_units, return_sequences=True))\n",
        "        model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "        return model\n",
        "\n",
        "    def prepare_data(self):\n",
        "        eng_tokenizer = self.create_tokenizer(self.dataset[:, 0])\n",
        "        self.eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "        self.eng_length = self.max_length(self.dataset[:, 0])\n",
        "        print('English Vocabulary Size: %d' % self.eng_vocab_size)\n",
        "        print('English Max Length: %d' % (self.eng_length))\n",
        "        spain_tokenizer = self.create_tokenizer(self.dataset[:, 1])\n",
        "        self.spain_vocab_size = len(spain_tokenizer.word_index) + 1\n",
        "        self.spain_length = self.max_length(self.dataset[:, 1])\n",
        "        print('Spain Vocabulary Size: %d' % self.spain_vocab_size)\n",
        "        print('Spain Max Length: %d' % (self.spain_length))\n",
        "        self.trainX = self.encode_sequences(spain_tokenizer, self.spain_length, self.train[:, 1])\n",
        "        trainY = self.encode_sequences(eng_tokenizer, self.eng_length, self.train[:, 0])\n",
        "        self.trainY = self.encode_output(trainY, self.eng_vocab_size)\n",
        "        self.testX = self.encode_sequences(spain_tokenizer, self.spain_length, self.test[:, 1])\n",
        "        testY = self.encode_sequences(eng_tokenizer, self.eng_length, self.test[:, 0])\n",
        "        self.testY = self.encode_output(testY, self.eng_vocab_size)\n",
        "\n",
        "    def train_model(self, filename_model='model_translation.h5'):\n",
        "        model = self.define_model(self.spain_vocab_size, self.eng_vocab_size, self.spain_length, self.eng_length,128)\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "        print(model.summary())\n",
        "        checkpoint = ModelCheckpoint(filename_model, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "        monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "        model.fit(self.trainX, self.trainY, epochs=50, batch_size=64, validation_data=(self.testX, self.testY), callbacks=[checkpoint,monitor], verbose=2)\n",
        "\n",
        "    def main_train(self):\n",
        "        self.split_dataset()\n",
        "        self.save_clean_data()\n",
        "        self.prepare_data()\n",
        "        self.train_model()\n",
        "\n",
        "train = TrainMachineTranslation()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved:  english-spanish-both.pkl english-spanish-train.pkl english-spanish-test.pkl\n",
            "English Vocabulary Size: 5240\n",
            "English Max Length: 4\n",
            "Spain Vocabulary Size: 9116\n",
            "Spain Max Length: 4\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 4, 128)            1166848   \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "repeat_vector_2 (RepeatVecto (None, 4, 128)            0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 4, 128)            131584    \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 4, 5240)           675960    \n",
            "=================================================================\n",
            "Total params: 2,105,976\n",
            "Trainable params: 2,105,976\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 19632 samples, validate on 4909 samples\n",
            "Epoch 1/50\n",
            " - 35s - loss: 6.2646 - val_loss: 5.7777\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 5.77769, saving model to model_translation.h5\n",
            "Epoch 2/50\n",
            " - 34s - loss: 5.5722 - val_loss: 5.5739\n",
            "\n",
            "Epoch 00002: val_loss improved from 5.77769 to 5.57393, saving model to model_translation.h5\n",
            "Epoch 3/50\n",
            " - 33s - loss: 5.4097 - val_loss: 5.4702\n",
            "\n",
            "Epoch 00003: val_loss improved from 5.57393 to 5.47016, saving model to model_translation.h5\n",
            "Epoch 4/50\n",
            " - 33s - loss: 5.3064 - val_loss: 5.4016\n",
            "\n",
            "Epoch 00004: val_loss improved from 5.47016 to 5.40164, saving model to model_translation.h5\n",
            "Epoch 5/50\n",
            " - 33s - loss: 5.2287 - val_loss: 5.3504\n",
            "\n",
            "Epoch 00005: val_loss improved from 5.40164 to 5.35045, saving model to model_translation.h5\n",
            "Epoch 6/50\n",
            " - 33s - loss: 5.1382 - val_loss: 5.2614\n",
            "\n",
            "Epoch 00006: val_loss improved from 5.35045 to 5.26143, saving model to model_translation.h5\n",
            "Epoch 7/50\n",
            " - 33s - loss: 5.0084 - val_loss: 5.1298\n",
            "\n",
            "Epoch 00007: val_loss improved from 5.26143 to 5.12982, saving model to model_translation.h5\n",
            "Epoch 8/50\n",
            " - 33s - loss: 4.8154 - val_loss: 4.9537\n",
            "\n",
            "Epoch 00008: val_loss improved from 5.12982 to 4.95374, saving model to model_translation.h5\n",
            "Epoch 9/50\n",
            " - 34s - loss: 4.6164 - val_loss: 4.8072\n",
            "\n",
            "Epoch 00009: val_loss improved from 4.95374 to 4.80725, saving model to model_translation.h5\n",
            "Epoch 10/50\n",
            " - 34s - loss: 4.4089 - val_loss: 4.6492\n",
            "\n",
            "Epoch 00010: val_loss improved from 4.80725 to 4.64915, saving model to model_translation.h5\n",
            "Epoch 11/50\n",
            " - 34s - loss: 4.1639 - val_loss: 4.4608\n",
            "\n",
            "Epoch 00011: val_loss improved from 4.64915 to 4.46078, saving model to model_translation.h5\n",
            "Epoch 12/50\n",
            " - 34s - loss: 3.9076 - val_loss: 4.2865\n",
            "\n",
            "Epoch 00012: val_loss improved from 4.46078 to 4.28651, saving model to model_translation.h5\n",
            "Epoch 13/50\n",
            " - 34s - loss: 3.6666 - val_loss: 4.1398\n",
            "\n",
            "Epoch 00013: val_loss improved from 4.28651 to 4.13978, saving model to model_translation.h5\n",
            "Epoch 14/50\n",
            " - 34s - loss: 3.4508 - val_loss: 4.0229\n",
            "\n",
            "Epoch 00014: val_loss improved from 4.13978 to 4.02287, saving model to model_translation.h5\n",
            "Epoch 15/50\n",
            " - 34s - loss: 3.2543 - val_loss: 3.9269\n",
            "\n",
            "Epoch 00015: val_loss improved from 4.02287 to 3.92688, saving model to model_translation.h5\n",
            "Epoch 16/50\n",
            " - 33s - loss: 3.0745 - val_loss: 3.8407\n",
            "\n",
            "Epoch 00016: val_loss improved from 3.92688 to 3.84070, saving model to model_translation.h5\n",
            "Epoch 17/50\n",
            " - 34s - loss: 2.9035 - val_loss: 3.7705\n",
            "\n",
            "Epoch 00017: val_loss improved from 3.84070 to 3.77047, saving model to model_translation.h5\n",
            "Epoch 18/50\n",
            " - 34s - loss: 2.7440 - val_loss: 3.6973\n",
            "\n",
            "Epoch 00018: val_loss improved from 3.77047 to 3.69730, saving model to model_translation.h5\n",
            "Epoch 19/50\n",
            " - 34s - loss: 2.5927 - val_loss: 3.6407\n",
            "\n",
            "Epoch 00019: val_loss improved from 3.69730 to 3.64070, saving model to model_translation.h5\n",
            "Epoch 20/50\n",
            " - 34s - loss: 2.4467 - val_loss: 3.5839\n",
            "\n",
            "Epoch 00020: val_loss improved from 3.64070 to 3.58394, saving model to model_translation.h5\n",
            "Epoch 21/50\n",
            " - 34s - loss: 2.3047 - val_loss: 3.5442\n",
            "\n",
            "Epoch 00021: val_loss improved from 3.58394 to 3.54418, saving model to model_translation.h5\n",
            "Epoch 22/50\n",
            " - 34s - loss: 2.1676 - val_loss: 3.4972\n",
            "\n",
            "Epoch 00022: val_loss improved from 3.54418 to 3.49723, saving model to model_translation.h5\n",
            "Epoch 23/50\n",
            " - 34s - loss: 2.0368 - val_loss: 3.4527\n",
            "\n",
            "Epoch 00023: val_loss improved from 3.49723 to 3.45270, saving model to model_translation.h5\n",
            "Epoch 24/50\n",
            " - 34s - loss: 1.9081 - val_loss: 3.4195\n",
            "\n",
            "Epoch 00024: val_loss improved from 3.45270 to 3.41946, saving model to model_translation.h5\n",
            "Epoch 25/50\n",
            " - 34s - loss: 1.7863 - val_loss: 3.3902\n",
            "\n",
            "Epoch 00025: val_loss improved from 3.41946 to 3.39018, saving model to model_translation.h5\n",
            "Epoch 26/50\n",
            " - 34s - loss: 1.6684 - val_loss: 3.3551\n",
            "\n",
            "Epoch 00026: val_loss improved from 3.39018 to 3.35509, saving model to model_translation.h5\n",
            "Epoch 27/50\n",
            " - 34s - loss: 1.5567 - val_loss: 3.3478\n",
            "\n",
            "Epoch 00027: val_loss improved from 3.35509 to 3.34782, saving model to model_translation.h5\n",
            "Epoch 28/50\n",
            " - 33s - loss: 1.4513 - val_loss: 3.3233\n",
            "\n",
            "Epoch 00028: val_loss improved from 3.34782 to 3.32334, saving model to model_translation.h5\n",
            "Epoch 29/50\n",
            " - 33s - loss: 1.3527 - val_loss: 3.3187\n",
            "\n",
            "Epoch 00029: val_loss improved from 3.32334 to 3.31874, saving model to model_translation.h5\n",
            "Epoch 30/50\n",
            " - 34s - loss: 1.2584 - val_loss: 3.3009\n",
            "\n",
            "Epoch 00030: val_loss improved from 3.31874 to 3.30087, saving model to model_translation.h5\n",
            "Epoch 31/50\n",
            " - 34s - loss: 1.1713 - val_loss: 3.2998\n",
            "\n",
            "Epoch 00031: val_loss improved from 3.30087 to 3.29985, saving model to model_translation.h5\n",
            "Epoch 32/50\n",
            " - 34s - loss: 1.0899 - val_loss: 3.3008\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 3.29985\n",
            "Epoch 33/50\n",
            " - 34s - loss: 1.0146 - val_loss: 3.2840\n",
            "\n",
            "Epoch 00033: val_loss improved from 3.29985 to 3.28401, saving model to model_translation.h5\n",
            "Epoch 34/50\n",
            " - 34s - loss: 0.9425 - val_loss: 3.2916\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 3.28401\n",
            "Epoch 35/50\n",
            " - 34s - loss: 0.8787 - val_loss: 3.3057\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 3.28401\n",
            "Epoch 36/50\n",
            " - 33s - loss: 0.8202 - val_loss: 3.3049\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 3.28401\n",
            "Epoch 37/50\n",
            " - 33s - loss: 0.7644 - val_loss: 3.3002\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 3.28401\n",
            "Epoch 38/50\n",
            " - 33s - loss: 0.7142 - val_loss: 3.3169\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 3.28401\n",
            "Epoch 00038: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePCqxzEU6-WQ",
        "colab_type": "code",
        "outputId": "ed8d1bad-c507-4f2e-c429-ad30415e7c26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 881
        }
      },
      "source": [
        "class EvaluateMachineTranslation:\n",
        "    def __init__(self, file_dataset='english-spanish-both.pkl', file_train='english-spanish-train.pkl', file_test='english-spanish-test.pkl'):\n",
        "        self.dataset = self.load_data(file_dataset)\n",
        "        self.train = self.load_data(file_train)\n",
        "        self.test = self.load_data(file_test)\n",
        "        self.eng_tokenizer = ''\n",
        "        self.trainX = ''\n",
        "        self.testX = ''\n",
        "        self.main_evaluate()\n",
        "\n",
        "    def load_data(self, filename):\n",
        "        return load(open(filename, 'rb'))\n",
        "\n",
        "    def create_tokenizer(self, lines):\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(lines)\n",
        "        return tokenizer\n",
        "\n",
        "    def max_length(self, lines):\n",
        "        return max(len(line.split()) for line in lines)\n",
        "\n",
        "    def encode_sequences(self, tokenizer, length, lines):\n",
        "        X = tokenizer.texts_to_sequences(lines)\n",
        "        X = pad_sequences(X, maxlen=length, padding='post')\n",
        "        return X\n",
        "\n",
        "    def word_for_id(self, integer, tokenizer):\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == integer:\n",
        "                return word\n",
        "        return None\n",
        "\n",
        "    def predict_sequence(self, model, tokenizer, source):\n",
        "        prediction = model.predict(source, verbose=0)[0]\n",
        "        integers = [argmax(vector) for vector in prediction]\n",
        "        target = list()\n",
        "        for i in integers:\n",
        "            word = self.word_for_id(i, tokenizer)\n",
        "            if word is None:\n",
        "                break\n",
        "            target.append(word)\n",
        "        return ' '.join(target)\n",
        "\n",
        "    def evaluate_model(self, model, tokenizer, sources, raw_dataset):\n",
        "        actual, predicted = list(), list()\n",
        "        for i, source in enumerate(sources):\n",
        "            source = source.reshape((1, source.shape[0]))\n",
        "            translation = self.predict_sequence(model, tokenizer, source)\n",
        "            raw_target, raw_src = raw_dataset[i]\n",
        "            if i in range(30,50):\n",
        "                print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "            actual.append(raw_target.split())\n",
        "            predicted.append(translation.split())\n",
        "        print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0), smoothing_function=SmoothingFunction().method7))\n",
        "        print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0), smoothing_function=SmoothingFunction().method7))\n",
        "        print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0), smoothing_function=SmoothingFunction().method7))\n",
        "        print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=SmoothingFunction().method7))\n",
        "\n",
        "    def prepare_data(self):\n",
        "        self.eng_tokenizer = self.create_tokenizer(self.dataset[:, 0])\n",
        "        eng_vocab_size = len(self.eng_tokenizer.word_index) + 1\n",
        "        eng_length = self.max_length(self.dataset[:, 0])\n",
        "        spain_tokenizer = self.create_tokenizer(self.dataset[:, 1])\n",
        "        spain_vocab_size = len(spain_tokenizer.word_index) + 1\n",
        "        spain_length = self.max_length(self.dataset[:, 1])\n",
        "        self.trainX = self.encode_sequences(spain_tokenizer, spain_length, self.train[:, 1])\n",
        "        self.testX = self.encode_sequences(spain_tokenizer, spain_length, self.test[:, 1])\n",
        "\n",
        "    def main_evaluate(self):\n",
        "        self.prepare_data()\n",
        "        model = load_model('model_translation.h5')\n",
        "        print('train')\n",
        "        self.evaluate_model(model, self.eng_tokenizer, self.trainX, self.train)\n",
        "        print('test')\n",
        "        self.evaluate_model(model, self.eng_tokenizer, self.testX, self.test)\n",
        "\n",
        "evaluate = EvaluateMachineTranslation()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[comeis carne], target=[do you eat meat], predicted=[do you eat meat]\n",
            "src=[esquiar es mi pasion], target=[skiing is my passion], predicted=[here is my passion]\n",
            "src=[ya termino], target=[its finished now], predicted=[is already already]\n",
            "src=[ella dio dinero], target=[she gave money], predicted=[she gave money money]\n",
            "src=[grite], target=[i screamed], predicted=[i screamed]\n",
            "src=[es demasiado temprano], target=[its too early], predicted=[its too early]\n",
            "src=[son lindos], target=[they are pretty], predicted=[they are pretty]\n",
            "src=[no me hagas quedar], target=[dont make me stay], predicted=[dont make me]\n",
            "src=[tom es un democrata], target=[tom is a democrat], predicted=[tom is a democrat]\n",
            "src=[ellos invadieron el pais], target=[they invaded the country], predicted=[they invaded the country]\n",
            "src=[estoy exhausto], target=[i am exhausted], predicted=[im exhausted]\n",
            "src=[tom conoce a mary], target=[tom has met mary], predicted=[tom wants mary mary]\n",
            "src=[banderas japonesas ondeaban], target=[japanese flags were flying], predicted=[lets flags were flying]\n",
            "src=[esto funcionara], target=[thisll work], predicted=[thisll work]\n",
            "src=[alguien pregunto por ti], target=[someone asked for you], predicted=[someone for for you]\n",
            "src=[puede que llueva pronto], target=[it may rain soon], predicted=[it may rain soon]\n",
            "src=[no es nada personal], target=[dont take it personally], predicted=[dont dont for you]\n",
            "src=[tom abordo el barco], target=[tom boarded the ship], predicted=[tom boarded the ship]\n",
            "src=[ella es una estudiante], target=[she is a student], predicted=[she is a student]\n",
            "src=[tom esta ocupado], target=[toms busy], predicted=[tom tom busy]\n",
            "BLEU-1: 0.430854\n",
            "BLEU-2: 0.330841\n",
            "BLEU-3: 0.307349\n",
            "BLEU-4: 0.212392\n",
            "test\n",
            "src=[las gemelas sonrieron], target=[the twins smiled], predicted=[the sang ready]\n",
            "src=[tom tiene tres novias], target=[tom has three girlfriends], predicted=[tom has three girlfriends]\n",
            "src=[podria mataros], target=[i could kill you], predicted=[i could kill you]\n",
            "src=[ha llegado la primavera], target=[spring has come], predicted=[spring has back]\n",
            "src=[esta cosa es tuya], target=[is that thing yours], predicted=[this this your yours]\n",
            "src=[es mas que suficiente], target=[its more than enough], predicted=[its time time time]\n",
            "src=[no soy tu novia], target=[im not your girlfriend], predicted=[im not my news]\n",
            "src=[mira al espejo], target=[look in the mirror], predicted=[look at your]\n",
            "src=[ella es pianista], target=[she is a pianist], predicted=[she is a]\n",
            "src=[estan todos listos], target=[are you all ready], predicted=[they all ready]\n",
            "src=[ordene una pizza], target=[i ordered a pizza], predicted=[i ordered art]\n",
            "src=[casi me pegas], target=[you almost hit me], predicted=[there already with me]\n",
            "src=[solo dejame solo], target=[just leave me alone], predicted=[just me alone alone]\n",
            "src=[cual era la diferencia], target=[what was the difference], predicted=[what was the plan]\n",
            "src=[habla por ti], target=[speak for yourself], predicted=[do you me]\n",
            "src=[he perdido mi billete], target=[ive lost my ticket], predicted=[ive my my]\n",
            "src=[tomemoslo con calma], target=[lets take it easy], predicted=[be carefully]\n",
            "src=[coge solo uno], target=[just take one], predicted=[just take one one]\n",
            "src=[tom reinicio su computadora], target=[tom restarted his computer], predicted=[tom is his bicycle]\n",
            "src=[el es muy canuto], target=[hes very stingy], predicted=[hes is very crazy]\n",
            "BLEU-1: 0.435515\n",
            "BLEU-2: 0.346018\n",
            "BLEU-3: 0.325809\n",
            "BLEU-4: 0.229653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewyQcQe8rPAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}