{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM Translation Eng to Spain",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU6tnqrN52z9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "147760dc-cda0-4c65-ce72-b35265c62555"
      },
      "source": [
        "import string\n",
        "import math\n",
        "import re\n",
        "from fractions import Fraction\n",
        "from pickle import dump,load\n",
        "from unicodedata import normalize\n",
        "from numpy import array,argmax\n",
        "from numpy.random import rand,shuffle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import LSTM,Dense,Embedding,RepeatVector,TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from keras import backend as K\n",
        "K.tensorflow_backend._get_available_gpus()"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVjgY8d856M8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Preprocessing:\n",
        "    def __init__(self, filename='translation_english.txt'):\n",
        "        self.document = self.load_doc(filename)\n",
        "        self.clean_document = ''\n",
        "\n",
        "    def load_doc(self, filename):\n",
        "        file = open(filename, mode='rt', encoding='utf-8')\n",
        "        text = file.read()\n",
        "        file.close()\n",
        "        return text\n",
        "\n",
        "    def clean_pairs(self, lines):\n",
        "        cleaned = list()\n",
        "        re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "        table = str.maketrans('', '', string.punctuation)\n",
        "        for pair in lines:\n",
        "            clean_pair = list()\n",
        "            for line in pair:\n",
        "                line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "                line = line.decode('UTF-8')\n",
        "                line = line.lower().split()\n",
        "                line = [word.translate(table) for word in line]\n",
        "                line = [re_print.sub('', word) for word in line]\n",
        "                line = [word for word in line if word.isalpha()]\n",
        "                clean_pair.append(' '.join(line))\n",
        "            cleaned.append(clean_pair)\n",
        "        self.clean_document = array(cleaned)\n",
        "        return array(cleaned)\n",
        "\n",
        "    def get_clean_pairs(self):\n",
        "        return self.clean_document\n",
        "\n",
        "    def to_pairs(self):\n",
        "        lines = self.document.strip().split('\\n')\n",
        "        pairs = [line.split('\\t') for line in  lines]\n",
        "        return pairs\n",
        "\n",
        "    def save_clean_data(self, document, filename='english-spanish.pkl'):\n",
        "        dump(document, open(filename, 'wb'))\n",
        "        print('Saved: %s' % filename)\n",
        "\n",
        "    def preprocess(self):\n",
        "        self.save_clean_data(\n",
        "            self.clean_pairs(\n",
        "                self.to_pairs()\n",
        "            )\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-9WxEPg5_mA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5c48efea-52e3-4d03-aef1-505fc60f85c5"
      },
      "source": [
        "preprocess = Preprocessing()\n",
        "preprocess.preprocess()\n",
        "for i in range(900,1000):\n",
        "\tprint('[%s] => [%s]' % (preprocess.get_clean_pairs()[i,0], preprocess.get_clean_pairs()[i,1]))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-spanish.pkl\n",
            "[be careful] => [ten cuidado]\n",
            "[be careful] => [se cuidadoso]\n",
            "[be content] => [estate contento]\n",
            "[be on time] => [llega a tiempo]\n",
            "[be on time] => [llegue a tiempo]\n",
            "[be patient] => [sea paciente]\n",
            "[be serious] => [se serio]\n",
            "[birds sing] => [los pajaros cantan]\n",
            "[birds sing] => [los pajaros estan cantando]\n",
            "[bring food] => [traed comida]\n",
            "[bring help] => [traed ayuda]\n",
            "[bring wine] => [trae vino]\n",
            "[can i come] => [puedo ir]\n",
            "[can i come] => [puedo venir]\n",
            "[can i come] => [puedo acercarme]\n",
            "[can i help] => [puedo ayudar]\n",
            "[can i stay] => [me puedo quedar]\n",
            "[carry this] => [lleva esto]\n",
            "[check that] => [comprobad eso]\n",
            "[check this] => [comprueba esto]\n",
            "[choose one] => [escoge uno]\n",
            "[come again] => [vuelve otra vez]\n",
            "[come alone] => [ven solo]\n",
            "[come along] => [vente]\n",
            "[come along] => [venganse]\n",
            "[come early] => [veni temprano]\n",
            "[come early] => [ven temprano]\n",
            "[come early] => [vengan temprano]\n",
            "[come early] => [venga temprano]\n",
            "[come on in] => [pasale]\n",
            "[come on in] => [pasele]\n",
            "[come on in] => [pasenle]\n",
            "[come on in] => [entre]\n",
            "[come on in] => [pase]\n",
            "[come quick] => [ven rapido]\n",
            "[come quick] => [veni rapido]\n",
            "[come to me] => [ven a mi]\n",
            "[come to me] => [venid a mi]\n",
            "[come to us] => [ven a nosotros]\n",
            "[come to us] => [venid a nosotros]\n",
            "[cut it out] => [ya parale]\n",
            "[did tom go] => [fue tom]\n",
            "[do come in] => [pasale]\n",
            "[do come in] => [pasele]\n",
            "[do come in] => [pasenle]\n",
            "[do come in] => [pasa adentro]\n",
            "[do come in] => [entra de una vez]\n",
            "[do come in] => [entra ya]\n",
            "[do come in] => [metete dentro]\n",
            "[do men cry] => [los hombres lloran]\n",
            "[dont come] => [no vengas]\n",
            "[dont jump] => [no salteis]\n",
            "[dont look] => [no mireis]\n",
            "[dont move] => [no os movais]\n",
            "[dont move] => [no te muevas]\n",
            "[dont move] => [no se mueva]\n",
            "[dont move] => [no se muevan]\n",
            "[dont sing] => [no cantes]\n",
            "[dont sing] => [no canten]\n",
            "[dont stop] => [no pares]\n",
            "[dont talk] => [no hables]\n",
            "[dont talk] => [no hables]\n",
            "[dont wait] => [no esperes]\n",
            "[dont wait] => [no esperen]\n",
            "[dont wait] => [no esperes]\n",
            "[dont yell] => [no grites]\n",
            "[eat slowly] => [come despacio]\n",
            "[eat slowly] => [come despacio]\n",
            "[fire burns] => [el fuego quema]\n",
            "[follow tom] => [seguilo a tomas]\n",
            "[follow tom] => [siguelo a tomas]\n",
            "[follow tom] => [sigalo a tomas]\n",
            "[follow tom] => [siganlo a tomas]\n",
            "[follow him] => [siguele]\n",
            "[follow him] => [siguelo]\n",
            "[forget tom] => [olvidate de tomas]\n",
            "[forget tom] => [olvidate de tomas]\n",
            "[forget tom] => [olvidese de tomas]\n",
            "[forget him] => [olvidenlo]\n",
            "[forgive us] => [perdonanos]\n",
            "[forgive us] => [perdonenos]\n",
            "[get a life] => [consiguete una vida]\n",
            "[get inside] => [entra]\n",
            "[get to bed] => [vete a la cama]\n",
            "[give it up] => [dejalo]\n",
            "[go on home] => [vete a casa]\n",
            "[go on home] => [vayase a casa]\n",
            "[go see tom] => [ve a ver a tom]\n",
            "[go to work] => [anda a trabajar]\n",
            "[god exists] => [dios existe]\n",
            "[have faith] => [ten fe]\n",
            "[have faith] => [tened fe]\n",
            "[have faith] => [tengan fe]\n",
            "[have faith] => [confien]\n",
            "[he ate out] => [el salio a comer]\n",
            "[he coughed] => [tosio]\n",
            "[he gave in] => [el se rindio]\n",
            "[he gave up] => [se rindio]\n",
            "[he gave up] => [lo dejo]\n",
            "[he gave up] => [cedio]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkGW4Pd06MxR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc75f11f-a675-4bb5-808b-541f7d99635f"
      },
      "source": [
        "class TrainMachineTranslation:\n",
        "    def __init__(self, file_dataset = 'english-spanish.pkl'):\n",
        "        self.dataset = self.load_data(file_dataset)\n",
        "        self.eng_vocab_size = 0\n",
        "        self.eng_length = 0\n",
        "        self.spain_vocab_size = 0\n",
        "        self.spain_length = 0\n",
        "        self.train = ''\n",
        "        self.test = ''\n",
        "        self.trainX = ''\n",
        "        self.trainY = ''\n",
        "        self.testX = ''\n",
        "        self.testY = ''\n",
        "        self.main_train()\n",
        "\n",
        "    def split_dataset(self):\n",
        "        raw_dataset = self.dataset\n",
        "        shuffle(raw_dataset)\n",
        "        new_dataset = []\n",
        "        for i in raw_dataset:\n",
        "            if len(i[1].split(\" \")) <= 4 and len(i[0].split(\" \")) <= 4:\n",
        "                new_dataset.append([i[0],i[1]])\n",
        "        new_dataset = array(new_dataset)\n",
        "        n_sentences = len(new_dataset)\n",
        "        self.dataset = new_dataset[:n_sentences, :]\n",
        "        shuffle(self.dataset)\n",
        "        split = math.floor(len(self.dataset) - (len(self.dataset)*0.2))\n",
        "        self.train, self.test = self.dataset[:split], self.dataset[split:]\n",
        "\n",
        "    def load_data(self, filename):\n",
        "\t      return load(open(filename, 'rb'))\n",
        "\n",
        "    def save_clean_data(self, filename_dataset = 'english-spanish-both.pkl', filename_test = 'english-spanish-test.pkl', filename_train = 'english-spanish-train.pkl'):\n",
        "        dump(self.dataset, open(filename_dataset, 'wb'))\n",
        "        dump(self.train, open(filename_train, 'wb'))\n",
        "        dump(self.test, open(filename_test, 'wb'))\n",
        "        print('Saved: ', filename_dataset, filename_train, filename_test)\n",
        "\n",
        "    def create_tokenizer(self, lines):\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(lines)\n",
        "        return tokenizer\n",
        "\n",
        "    def max_length(self, lines):\n",
        "        return max(len(line.split()) for line in lines)\n",
        "\n",
        "    def encode_sequences(self, tokenizer, length, lines):\n",
        "        X = tokenizer.texts_to_sequences(lines)\n",
        "        X = pad_sequences(X, maxlen=length, padding='post')\n",
        "        return X\n",
        " \n",
        "    def encode_output(self, sequences, vocab_size):\n",
        "        ylist = [to_categorical(sequence, num_classes=vocab_size) for sequence in sequences]\n",
        "        y = array(ylist).reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "        return y\n",
        "\n",
        "    def define_model(self, src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "        model.add(LSTM(n_units))\n",
        "        model.add(RepeatVector(tar_timesteps))\n",
        "        model.add(LSTM(n_units, return_sequences=True))\n",
        "        model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "        return model\n",
        "\n",
        "    def prepare_data(self):\n",
        "        eng_tokenizer = self.create_tokenizer(self.dataset[:, 0])\n",
        "        self.eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "        self.eng_length = self.max_length(self.dataset[:, 0])\n",
        "        print('English Vocabulary Size: %d' % self.eng_vocab_size)\n",
        "        print('English Max Length: %d' % (self.eng_length))\n",
        "        spain_tokenizer = self.create_tokenizer(self.dataset[:, 1])\n",
        "        self.spain_vocab_size = len(spain_tokenizer.word_index) + 1\n",
        "        self.spain_length = self.max_length(self.dataset[:, 1])\n",
        "        print('Spain Vocabulary Size: %d' % self.spain_vocab_size)\n",
        "        print('Spain Max Length: %d' % (self.spain_length))\n",
        "        self.trainX = self.encode_sequences(spain_tokenizer, self.spain_length, self.train[:, 1])\n",
        "        trainY = self.encode_sequences(eng_tokenizer, self.eng_length, self.train[:, 0])\n",
        "        self.trainY = self.encode_output(trainY, self.eng_vocab_size)\n",
        "        self.testX = self.encode_sequences(spain_tokenizer, self.spain_length, self.test[:, 1])\n",
        "        testY = self.encode_sequences(eng_tokenizer, self.eng_length, self.test[:, 0])\n",
        "        self.testY = self.encode_output(testY, self.eng_vocab_size)\n",
        "\n",
        "    def train_model(self, filename_model='model_translation.h5'):\n",
        "        model = self.define_model(self.spain_vocab_size, self.eng_vocab_size, self.spain_length, self.eng_length,128)\n",
        "        model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "        print(model.summary())\n",
        "        checkpoint = ModelCheckpoint(filename_model, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "        monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
        "        model.fit(self.trainX, self.trainY, epochs=50, batch_size=64, validation_data=(self.testX, self.testY), callbacks=[checkpoint,monitor], verbose=2)\n",
        "\n",
        "    def main_train(self):\n",
        "        self.split_dataset()\n",
        "        self.save_clean_data()\n",
        "        self.prepare_data()\n",
        "        self.train_model()\n",
        "\n",
        "train = TrainMachineTranslation()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved:  english-spanish-both.pkl english-spanish-train.pkl english-spanish-test.pkl\n",
            "English Vocabulary Size: 5240\n",
            "English Max Length: 4\n",
            "Spain Vocabulary Size: 9116\n",
            "Spain Max Length: 4\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 4, 128)            1166848   \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "repeat_vector_2 (RepeatVecto (None, 4, 128)            0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 4, 128)            131584    \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 4, 5240)           675960    \n",
            "=================================================================\n",
            "Total params: 2,105,976\n",
            "Trainable params: 2,105,976\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 19632 samples, validate on 4909 samples\n",
            "Epoch 1/50\n",
            " - 33s - loss: 6.2815 - val_loss: 5.8300\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 5.83004, saving model to model_translation.h5\n",
            "Epoch 2/50\n",
            " - 31s - loss: 5.6220 - val_loss: 5.6403\n",
            "\n",
            "Epoch 00002: val_loss improved from 5.83004 to 5.64032, saving model to model_translation.h5\n",
            "Epoch 3/50\n",
            " - 31s - loss: 5.4601 - val_loss: 5.5324\n",
            "\n",
            "Epoch 00003: val_loss improved from 5.64032 to 5.53241, saving model to model_translation.h5\n",
            "Epoch 4/50\n",
            " - 31s - loss: 5.3437 - val_loss: 5.4524\n",
            "\n",
            "Epoch 00004: val_loss improved from 5.53241 to 5.45239, saving model to model_translation.h5\n",
            "Epoch 5/50\n",
            " - 32s - loss: 5.2500 - val_loss: 5.3910\n",
            "\n",
            "Epoch 00005: val_loss improved from 5.45239 to 5.39096, saving model to model_translation.h5\n",
            "Epoch 6/50\n",
            " - 32s - loss: 5.1457 - val_loss: 5.2928\n",
            "\n",
            "Epoch 00006: val_loss improved from 5.39096 to 5.29284, saving model to model_translation.h5\n",
            "Epoch 7/50\n",
            " - 32s - loss: 4.9908 - val_loss: 5.1425\n",
            "\n",
            "Epoch 00007: val_loss improved from 5.29284 to 5.14246, saving model to model_translation.h5\n",
            "Epoch 8/50\n",
            " - 32s - loss: 4.7843 - val_loss: 4.9710\n",
            "\n",
            "Epoch 00008: val_loss improved from 5.14246 to 4.97098, saving model to model_translation.h5\n",
            "Epoch 9/50\n",
            " - 32s - loss: 4.5678 - val_loss: 4.8041\n",
            "\n",
            "Epoch 00009: val_loss improved from 4.97098 to 4.80414, saving model to model_translation.h5\n",
            "Epoch 10/50\n",
            " - 32s - loss: 4.3492 - val_loss: 4.6299\n",
            "\n",
            "Epoch 00010: val_loss improved from 4.80414 to 4.62994, saving model to model_translation.h5\n",
            "Epoch 11/50\n",
            " - 33s - loss: 4.1055 - val_loss: 4.4477\n",
            "\n",
            "Epoch 00011: val_loss improved from 4.62994 to 4.44773, saving model to model_translation.h5\n",
            "Epoch 12/50\n",
            " - 32s - loss: 3.8534 - val_loss: 4.2826\n",
            "\n",
            "Epoch 00012: val_loss improved from 4.44773 to 4.28260, saving model to model_translation.h5\n",
            "Epoch 13/50\n",
            " - 32s - loss: 3.6164 - val_loss: 4.1499\n",
            "\n",
            "Epoch 00013: val_loss improved from 4.28260 to 4.14993, saving model to model_translation.h5\n",
            "Epoch 14/50\n",
            " - 33s - loss: 3.4013 - val_loss: 4.0361\n",
            "\n",
            "Epoch 00014: val_loss improved from 4.14993 to 4.03611, saving model to model_translation.h5\n",
            "Epoch 15/50\n",
            " - 31s - loss: 3.2023 - val_loss: 3.9321\n",
            "\n",
            "Epoch 00015: val_loss improved from 4.03611 to 3.93210, saving model to model_translation.h5\n",
            "Epoch 16/50\n",
            " - 31s - loss: 3.0222 - val_loss: 3.8540\n",
            "\n",
            "Epoch 00016: val_loss improved from 3.93210 to 3.85396, saving model to model_translation.h5\n",
            "Epoch 17/50\n",
            " - 31s - loss: 2.8555 - val_loss: 3.7782\n",
            "\n",
            "Epoch 00017: val_loss improved from 3.85396 to 3.77818, saving model to model_translation.h5\n",
            "Epoch 18/50\n",
            " - 32s - loss: 2.6978 - val_loss: 3.7147\n",
            "\n",
            "Epoch 00018: val_loss improved from 3.77818 to 3.71473, saving model to model_translation.h5\n",
            "Epoch 19/50\n",
            " - 32s - loss: 2.5462 - val_loss: 3.6512\n",
            "\n",
            "Epoch 00019: val_loss improved from 3.71473 to 3.65121, saving model to model_translation.h5\n",
            "Epoch 20/50\n",
            " - 32s - loss: 2.3984 - val_loss: 3.5959\n",
            "\n",
            "Epoch 00020: val_loss improved from 3.65121 to 3.59593, saving model to model_translation.h5\n",
            "Epoch 21/50\n",
            " - 32s - loss: 2.2583 - val_loss: 3.5514\n",
            "\n",
            "Epoch 00021: val_loss improved from 3.59593 to 3.55137, saving model to model_translation.h5\n",
            "Epoch 22/50\n",
            " - 32s - loss: 2.1210 - val_loss: 3.5120\n",
            "\n",
            "Epoch 00022: val_loss improved from 3.55137 to 3.51201, saving model to model_translation.h5\n",
            "Epoch 23/50\n",
            " - 32s - loss: 1.9914 - val_loss: 3.4808\n",
            "\n",
            "Epoch 00023: val_loss improved from 3.51201 to 3.48077, saving model to model_translation.h5\n",
            "Epoch 24/50\n",
            " - 32s - loss: 1.8664 - val_loss: 3.4515\n",
            "\n",
            "Epoch 00024: val_loss improved from 3.48077 to 3.45146, saving model to model_translation.h5\n",
            "Epoch 25/50\n",
            " - 32s - loss: 1.7477 - val_loss: 3.4233\n",
            "\n",
            "Epoch 00025: val_loss improved from 3.45146 to 3.42333, saving model to model_translation.h5\n",
            "Epoch 26/50\n",
            " - 32s - loss: 1.6357 - val_loss: 3.4005\n",
            "\n",
            "Epoch 00026: val_loss improved from 3.42333 to 3.40054, saving model to model_translation.h5\n",
            "Epoch 27/50\n",
            " - 32s - loss: 1.5278 - val_loss: 3.3788\n",
            "\n",
            "Epoch 00027: val_loss improved from 3.40054 to 3.37884, saving model to model_translation.h5\n",
            "Epoch 28/50\n",
            " - 32s - loss: 1.4250 - val_loss: 3.3607\n",
            "\n",
            "Epoch 00028: val_loss improved from 3.37884 to 3.36070, saving model to model_translation.h5\n",
            "Epoch 29/50\n",
            " - 33s - loss: 1.3279 - val_loss: 3.3585\n",
            "\n",
            "Epoch 00029: val_loss improved from 3.36070 to 3.35847, saving model to model_translation.h5\n",
            "Epoch 30/50\n",
            " - 32s - loss: 1.2385 - val_loss: 3.3420\n",
            "\n",
            "Epoch 00030: val_loss improved from 3.35847 to 3.34204, saving model to model_translation.h5\n",
            "Epoch 31/50\n",
            " - 32s - loss: 1.1534 - val_loss: 3.3372\n",
            "\n",
            "Epoch 00031: val_loss improved from 3.34204 to 3.33717, saving model to model_translation.h5\n",
            "Epoch 32/50\n",
            " - 32s - loss: 1.0723 - val_loss: 3.3204\n",
            "\n",
            "Epoch 00032: val_loss improved from 3.33717 to 3.32041, saving model to model_translation.h5\n",
            "Epoch 33/50\n",
            " - 32s - loss: 0.9974 - val_loss: 3.3257\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 3.32041\n",
            "Epoch 34/50\n",
            " - 32s - loss: 0.9298 - val_loss: 3.3360\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 3.32041\n",
            "Epoch 35/50\n",
            " - 32s - loss: 0.8657 - val_loss: 3.3329\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 3.32041\n",
            "Epoch 36/50\n",
            " - 31s - loss: 0.8083 - val_loss: 3.3387\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 3.32041\n",
            "Epoch 37/50\n",
            " - 31s - loss: 0.7547 - val_loss: 3.3518\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 3.32041\n",
            "Epoch 00037: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePCqxzEU6-WQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 970
        },
        "outputId": "02155d0d-d9e1-4d63-da62-b64eec585404"
      },
      "source": [
        "class EvaluateMachineTranslation:\n",
        "    def __init__(self, file_dataset='english-spanish-both.pkl', file_train='english-spanish-train.pkl', file_test='english-spanish-test.pkl'):\n",
        "        self.dataset = self.load_data(file_dataset)\n",
        "        self.train = self.load_data(file_train)\n",
        "        self.test = self.load_data(file_test)\n",
        "        self.eng_tokenizer = ''\n",
        "        self.trainX = ''\n",
        "        self.testX = ''\n",
        "        self.main_evaluate()\n",
        "\n",
        "    def load_data(self, filename):\n",
        "        return load(open(filename, 'rb'))\n",
        "\n",
        "    def create_tokenizer(self, lines):\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(lines)\n",
        "        return tokenizer\n",
        "\n",
        "    def max_length(self, lines):\n",
        "        return max(len(line.split()) for line in lines)\n",
        "\n",
        "    def encode_sequences(self, tokenizer, length, lines):\n",
        "        X = tokenizer.texts_to_sequences(lines)\n",
        "        X = pad_sequences(X, maxlen=length, padding='post')\n",
        "        return X\n",
        "\n",
        "    def word_for_id(self, integer, tokenizer):\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == integer:\n",
        "                return word\n",
        "        return None\n",
        "\n",
        "    def predict_sequence(self, model, tokenizer, source):\n",
        "        prediction = model.predict(source, verbose=0)[0]\n",
        "        integers = [argmax(vector) for vector in prediction]\n",
        "        target = list()\n",
        "        for i in integers:\n",
        "            word = word_for_id(i, tokenizer)\n",
        "            if word is None:\n",
        "                break\n",
        "            target.append(word)\n",
        "        return ' '.join(target)\n",
        "\n",
        "    def evaluate_model(self, model, tokenizer, sources, raw_dataset):\n",
        "        actual, predicted = list(), list()\n",
        "        for i, source in enumerate(sources):\n",
        "            source = source.reshape((1, source.shape[0]))\n",
        "            translation = self.predict_sequence(model, eng_tokenizer, source)\n",
        "            raw_target, raw_src = raw_dataset[i]\n",
        "            if i in range(30,50):\n",
        "                print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "            actual.append(raw_target.split())\n",
        "            predicted.append(translation.split())\n",
        "        print('BLEU-1: %f' % Fraction(corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))))\n",
        "        print('BLEU-2: %f' % Fraction(corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))))\n",
        "        print('BLEU-3: %f' % Fraction(corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0))))\n",
        "        print('BLEU-4: %f' % Fraction(corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))))\n",
        "\n",
        "    def prepare_data(self):\n",
        "        self.eng_tokenizer = self.create_tokenizer(self.dataset[:, 0])\n",
        "        eng_vocab_size = len(self.eng_tokenizer.word_index) + 1\n",
        "        eng_length = self.max_length(self.dataset[:, 0])\n",
        "        spain_tokenizer = self.create_tokenizer(self.dataset[:, 1])\n",
        "        spain_vocab_size = len(spain_tokenizer.word_index) + 1\n",
        "        spain_length = self.max_length(self.dataset[:, 1])\n",
        "        self.trainX = self.encode_sequences(spain_tokenizer, spain_length, self.train[:, 1])\n",
        "        self.testX = self.encode_sequences(spain_tokenizer, spain_length, self.test[:, 1])\n",
        "\n",
        "    def main_evaluate(self):\n",
        "        self.prepare_data()\n",
        "        model = load_model('model_translation.h5')\n",
        "        print('train')\n",
        "        self.evaluate_model(model, self.eng_tokenizer, self.trainX, self.train)\n",
        "        print('test')\n",
        "        self.evaluate_model(model, self.eng_tokenizer, self.testX, self.test)\n",
        "\n",
        "evaluate = EvaluateMachineTranslation()"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[llevaselo a tom], target=[take it to tom], predicted=[stop it were tom]\n",
            "src=[estoy con diarrea], target=[ive got diarrhea], predicted=[love hes preparing]\n",
            "src=[soy un perdedor], target=[im a loser], predicted=[im a rude]\n",
            "src=[eso es plastico], target=[thats plastic], predicted=[thats quarreled]\n",
            "src=[parecia joven], target=[he appeared young], predicted=[he heard smart]\n",
            "src=[cuantos anos tiene], target=[whats your age], predicted=[what are is our]\n",
            "src=[estas herida], target=[youre wounded], predicted=[are you]\n",
            "src=[yo necesito agua], target=[i need water], predicted=[i am had]\n",
            "src=[no fume aqui], target=[dont smoke here], predicted=[dont of here]\n",
            "src=[es un poco debil], target=[its a bit flimsy], predicted=[its a exhausted led]\n",
            "src=[comio una manzana], target=[she ate one apple], predicted=[do did a hi]\n",
            "src=[como te sientes], target=[how do you feel], predicted=[how she you today]\n",
            "src=[no te quejes], target=[dont complain], predicted=[dont complain]\n",
            "src=[mandalo adentro], target=[send him in], predicted=[empty him in]\n",
            "src=[tom es un ladron], target=[tom is a thief], predicted=[tom is a pathetic]\n",
            "src=[me encanta esta foto], target=[i love this photo], predicted=[i good this wears]\n",
            "src=[increible a que si], target=[isnt that terrific], predicted=[thats that must]\n",
            "src=[donde queda el zoologico], target=[wheres the zoo], predicted=[tell the oysters]\n",
            "src=[hasta manana], target=[see you tomorrow], predicted=[you you shes]\n",
            "src=[venga con nosotros], target=[come with us], predicted=[come at at]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU-1: 0.060069\n",
            "BLEU-2: 0.244052\n",
            "BLEU-3: 0.427578\n",
            "BLEU-4: 0.491925\n",
            "test\n",
            "src=[estoy apurado], target=[im in a hurry], predicted=[i want]\n",
            "src=[los caballos son animales], target=[horses are animals], predicted=[the are loved]\n",
            "src=[reunamonos manana], target=[lets get together tomorrow], predicted=[shes shes shes]\n",
            "src=[ya estoy enferma], target=[im already sick], predicted=[i toms toms]\n",
            "src=[soy de australia], target=[im from australia], predicted=[im understand understand]\n",
            "src=[es muy talentoso], target=[he is very talented], predicted=[got very very]\n",
            "src=[ella rechazo su oferta], target=[she refused his offer], predicted=[do fly lets cake]\n",
            "src=[me quede asombrado], target=[i was astonished], predicted=[i dog attended]\n",
            "src=[vuelve a la cama], target=[go back to bed], predicted=[go in were]\n",
            "src=[ese es mi auto], target=[thats my car], predicted=[that we we wasnt]\n",
            "src=[yo jugare con ustedes], target=[ill play with you], predicted=[ill started you you]\n",
            "src=[tomas esta bromeando], target=[tom is kidding], predicted=[tom is]\n",
            "src=[esta informacion es confidencial], target=[this information is confidential], predicted=[this is pay pay]\n",
            "src=[tom esta disgustado], target=[tom is upset], predicted=[tom is fine]\n",
            "src=[el no es estupido], target=[he is not stupid], predicted=[got not three]\n",
            "src=[mirame], target=[look at me], predicted=[stop me]\n",
            "src=[no paren de tocar], target=[dont stop playing], predicted=[dont really speak speak]\n",
            "src=[no te muevas], target=[stop moving], predicted=[dont not]\n",
            "src=[podrias verificar eso], target=[can you verify that], predicted=[you leave dangerous that]\n",
            "src=[vamos a jugar], target=[lets play something], predicted=[please really us]\n",
            "BLEU-1: 0.056971\n",
            "BLEU-2: 0.237983\n",
            "BLEU-3: 0.421603\n",
            "BLEU-4: 0.486400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "035yf1cjxA5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}